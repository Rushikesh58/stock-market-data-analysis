# IBM Stock ETL Pipeline with Airflow, PySpark, and Docker

## ğŸ“Œ Project Overview

This project is an end-to-end ETL pipeline that extracts daily IBM stock data from the [Alpha Vantage API](https://www.alphavantage.co/), transforms and cleans the data using PySpark, and uploads the processed data to an AWS S3 bucket. The workflow is orchestrated with **Apache Airflow**, and all components run inside **Docker containers** using Docker Compose.

---

## ğŸ”§ Technologies Used

- **Apache Airflow** â€“ Workflow orchestration
- **PySpark** â€“ Data transformation and cleaning
- **Alpha Vantage API** â€“ Source for daily stock data
- **AWS S3** â€“ Destination for cleaned data
- **Docker & Docker Compose** â€“ Containerization and orchestration

---

## ğŸ”„ ETL Workflow

1. **Extract**:  
   The DAG calls the Alpha Vantage API to download daily stock data for IBM and saves it as a raw CSV file.

2. **Transform**:  
   A PySpark job:
   - Drops rows with missing values in key columns
   - Casts columns to appropriate data types
   - Adds a new column for daily change percentage

3. **Load**:  
   The transformed data is written to a CSV file and uploaded to an S3 bucket.

---

## ğŸ“ Project Structure

â”œâ”€â”€ dags

â”‚ â”œâ”€â”€ stock_etl_dag.py 

â”‚ â””â”€â”€ transform_stock_data.py

â”œâ”€â”€ requirements.txt # Python dependencies

â”œâ”€â”€ Dockerfile # Custom Airflow image

â””â”€â”€ docker-compose.yml # Multi-container setup

ğŸ›  Future Improvements
- Add retry logic for API failures
- Enable daily scheduling in Airflow
- Add tests for PySpark transformations
- Implement logging and monitoring
